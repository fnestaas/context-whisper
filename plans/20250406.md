## Current state
- model class works
- simple training works
- no useful training yet

## Target state
- Trained model on sensible data
- Used GPU to train a model with long context and prompts

## Steps needed
- decide on which data to use
- Get prompts (description) for each data batch
- Figure out where to train
- Extend current training to 
    - be more maintainable code
    - Fulfil the above requirements

## Plan
- Look for examples to work from online
- Has anyone tried to fine tune whisper on long audio with long context?
- Write a script containing data processing functions
    - Concatenating audio
    - providing tokenized transcription
    - modularizing the simple trianing in the notebook (data processing functions and data loader)
- Debug with other whisper sizes (flexibility with pretrained dimensions...)

## Notes
### [HF Inference tutorial](https://colab.research.google.com/drive/1l290cRv4RdvuLNlSeo9WexByHaNWs3s3?usp=sharing)
- HF datasets with streaming to reduce memory requirements (?)
- Huggingface has a chunking function for long audio

### Misc
- Might want to use whisper tiny for first experiments, not whisper small
- not sure what happens if d_model != 768 - does the code handle this? What about pretrained models (maybe we add an extra linear layer)? 
