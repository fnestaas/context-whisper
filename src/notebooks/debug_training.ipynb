{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Debug Training\n",
        "In this notebook, we \n",
        "- ensure that training works (in principle) with `ContextWhisperForCausalLM`, checking that no errors are thrown during the training loop.\n",
        "- Overfit on a single sample, validating that training works as expected\n",
        "- Freeze all parameters except that of the `text_encoder`, validating that the `text_encoder` signal is (at least somewhat) useful.\n",
        "\n",
        "\n",
        "The notebook was developed in Google Colab, granting access to GPU resources for small experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "g9VgTy_4XhPw",
        "outputId": "83d3c386-cc87-4672-e996-24c1968378c4"
      },
      "outputs": [],
      "source": [
        "!git clone https://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:x-oauth-basic@github.com/fnestaas/context-whisper.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JUNRbFGudUll",
        "outputId": "6ac2c96b-d570-428e-9182-8bfe94a5c3ad"
      },
      "outputs": [],
      "source": [
        "!pip install pdm uv\n",
        "!pdm config use_uv true\n",
        "!cp -r context-whisper/src/context_whisper ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Vrv3WQKQgSbt",
        "outputId": "4c1b2abb-6dc2-48db-b009-b11f1d90cda3"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!uv pip install --upgrade datasets[audio] transformers accelerate evaluate jiwer tensorboard gradio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya24XiH-mk5v",
        "outputId": "8e0a5ee3-8b5a-442b-b743-c6328273bbed"
      },
      "outputs": [],
      "source": [
        "!uv pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load a small dataset for debugging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WWs8Jk2hvTm",
        "outputId": "736b0fbc-942a-47ed-ca7c-1b2607e55b3d"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"rodoggx/ATCO2-ASR-1h\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8prXcx59nCDF"
      },
      "outputs": [],
      "source": [
        "from context_whisper.modules import ContextWhisperModel, ContextWhisperConfig, ContextWhisperForCausalLM\n",
        "from context_whisper.processing import ContextWhisperProcessor\n",
        "import torch\n",
        "from transformers.models.whisper.feature_extraction_whisper import WhisperFeatureExtractor\n",
        "from transformers.models.whisper.tokenization_whisper import WhisperTokenizer\n",
        "from transformers.models.bert.tokenization_bert import BertTokenizer\n",
        "\n",
        "whisper_str = 'openai/whisper-small'\n",
        "bert_str = 'google-bert/bert-base-uncased'\n",
        "\n",
        "tokenizer = WhisperTokenizer.from_pretrained(whisper_str)\n",
        "prompt_tokenizer = BertTokenizer.from_pretrained(bert_str)\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(whisper_str)\n",
        "\n",
        "processor = ContextWhisperProcessor(\n",
        "    tokenizer=tokenizer,\n",
        "    prompt_tokenizer=prompt_tokenizer,\n",
        "    feature_extractor=feature_extractor\n",
        ")\n",
        "\n",
        "config = ContextWhisperConfig(\n",
        "    d_model=768,\n",
        "    whisper_pretrained_str=whisper_str,\n",
        "    text_encoder_pretrained_str=bert_str\n",
        ")\n",
        "\n",
        "model = ContextWhisperForCausalLM(config).to('cuda')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Process the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n54p0cN1A3j7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "AdamW = torch.optim.AdamW\n",
        "\n",
        "train_dataset = ds['train']\n",
        "val_dataset = ds['test']\n",
        "\n",
        "# Data preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    # Get the audio features and tokenized input text\n",
        "    audio_features = processor(audio=examples['audio']['array'], sampling_rate=16000)\n",
        "    text_tokens = processor(text=examples['text_Str'], padding=True, truncation=True)  # Text data\n",
        "    prompt_tokens = processor(prompt='This is a recording about a fabulous view', padding=True, truncation=True)  # Example prompt\n",
        "\n",
        "    # Convert data into tensors\n",
        "    input_features = torch.tensor(audio_features['input_features']).to('cuda')\n",
        "    input_ids = torch.tensor(text_tokens['input_ids']).to('cuda')\n",
        "    prompt_ids = torch.tensor(prompt_tokens['input_ids']).to('cuda')\n",
        "\n",
        "    return {'input_features': input_features.squeeze(), 'input_ids': input_ids.squeeze(), 'prompt_ids': prompt_ids.squeeze()}\n",
        "\n",
        "# Apply the preprocessing function\n",
        "val_dataset = val_dataset.map(preprocess_function)\n",
        "train_dataset = train_dataset.map(preprocess_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Debugging: overfit on one sample\n",
        "Here we \n",
        "- take a single sample from the train data, and try to overfit on it, monitoring the loss on that sample by setting it to also be the validation data\n",
        "- freeze the `decoder` and `spectrogram_encoder`, leaving only the `text_encoder` changable during training. \n",
        "If the loss improves, it is due to the `text_encoder` updating its parameters.\n",
        "Note that we could make similar experiments with freezing other modules too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypDzQUGphNPC"
      },
      "outputs": [],
      "source": [
        "# overfit on one sample\n",
        "from datasets import Dataset\n",
        "val_dataset = Dataset.from_dict(train_dataset[[0]])\n",
        "train_dataset = Dataset.from_dict(train_dataset[[0]])\n",
        "\n",
        "# does the text_encoder matter?\n",
        "model.freeze_module(\"output_embeddings\")\n",
        "model.freeze_module(\"decoder\")\n",
        "model.freeze_module(\"spectrogram_encoder\")\n",
        "\n",
        "\n",
        "def model2params(m: torch.nn.Module):\n",
        "    return torch.concat([p.flatten() for p in m.parameters()]).detach().cpu().numpy()\n",
        "pre_training_params = model2params(model)\n",
        "pre_training_encoder = model2params(model.get_text_encoder()) # this, and nothing else, should change in this experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training setup and training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTkL0RFWFSTJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function to pad text and audio data for batching.\n",
        "\n",
        "    Args:\n",
        "        batch (list): List of samples from the dataset.\n",
        "\n",
        "    Returns:\n",
        "        dict: Batched data with padded text sequences and audio features.\n",
        "    \"\"\"\n",
        "    # Initialize lists for the batched data\n",
        "    input_features = []\n",
        "    input_ids = []\n",
        "    prompt_ids = []\n",
        "\n",
        "    # Iterate over each sample in the batch\n",
        "    for sample in batch:\n",
        "        input_features.append(torch.tensor(sample['input_features']))\n",
        "        input_ids.append(torch.tensor(sample['input_ids']).squeeze(0))  # remove unnecessary extra dimension\n",
        "        prompt_ids.append(torch.tensor(sample['prompt_ids']).squeeze(0))  # remove unnecessary extra dimension\n",
        "\n",
        "    # Pad the text sequences (input_ids and prompt_ids) to the max length in the batch\n",
        "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)  # Pad text tokens\n",
        "    prompt_ids = pad_sequence(prompt_ids, batch_first=True, padding_value=0)  # Pad prompt tokens\n",
        "\n",
        "    # Stack the audio features (input_features) into a tensor and pad them if necessary\n",
        "    input_features = torch.stack(input_features, dim=0)  # Stack audio features along the batch dimension\n",
        "\n",
        "    # Return the batch data in a dictionary\n",
        "    return {\n",
        "        'input_features': input_features,\n",
        "        'input_ids': input_ids,\n",
        "        'prompt_ids': prompt_ids\n",
        "    }\n",
        "# Setup DataLoader for training and validation\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 1\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIzNJVN3Cf3Z",
        "outputId": "0a48d327-af48-4ae5-c7ba-5bfcd042b842"
      },
      "outputs": [],
      "source": [
        "# Define optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    loop = tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}')\n",
        "    total_loss = 0\n",
        "    for batch in loop:\n",
        "        # Move inputs to GPU\n",
        "        input_features = batch['input_features'].to('cuda')\n",
        "        input_ids = batch['input_ids'].to('cuda')\n",
        "        prompt_ids = batch['prompt_ids'].to('cuda')\n",
        "\n",
        "        # Forward pass\n",
        "        encoder_out = model.get_encoder().forward(\n",
        "            spectrogram_input_features=input_features,\n",
        "            output_hidden_states=True,\n",
        "            text_encoder_input_ids=prompt_ids,\n",
        "        )\n",
        "        outputs = model(\n",
        "            decoder_input_ids=input_ids,\n",
        "            encoder_outputs=encoder_out,\n",
        "            output_hidden_states=True,\n",
        "            labels=input_ids\n",
        "          )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loop.set_postfix(loss=total_loss / (loop.n + 1))\n",
        "\n",
        "    # Evaluate after each epoch\n",
        "    model.eval()\n",
        "    eval_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_features = batch['input_features'].to('cuda')\n",
        "            input_ids = batch['input_ids'].to('cuda')\n",
        "            prompt_ids = batch['prompt_ids'].to('cuda')\n",
        "\n",
        "            enc_out = model.get_encoder().forward(\n",
        "                spectrogram_input_features=input_features,\n",
        "                output_hidden_states=True,\n",
        "                text_encoder_input_ids=prompt_ids,\n",
        "            )\n",
        "            # Forward pass (without calculating gradients)\n",
        "            outputs = model(\n",
        "                decoder_input_ids=input_ids,\n",
        "                encoder_outputs=enc_out,\n",
        "                output_hidden_states=True,\n",
        "                labels=input_ids\n",
        "            )\n",
        "\n",
        "            eval_loss += outputs.loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch + 1} - Eval Loss: {eval_loss / len(val_dataloader)}')\n",
        "\n",
        "# Optionally save the model after training\n",
        "model.save_pretrained('./context_whisper_model', safe_serialization=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validate that training only changed the `text_encoder`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5MCpdR9LNkY"
      },
      "outputs": [],
      "source": [
        "post_training_params = model2params(model)\n",
        "train_param_diff = post_training_params - pre_training_params\n",
        "nz_diff = train_param_diff.nonzero()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCEfKupZ8S_L"
      },
      "outputs": [],
      "source": [
        "assert len(nz_diff) <= len(pre_training_encoder), \"Too many parameters changed\"\n",
        "assert nz_diff.max() - nz_diff.min() < len(pre_training_encoder), \"Too many parameters changed\"\n",
        "assert len(post_training_params) > len(pre_training_encoder), \"All parameters can be changed\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2-z37HF8xxz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
